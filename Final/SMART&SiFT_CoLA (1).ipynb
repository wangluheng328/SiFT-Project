{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMART&SiFT_CoLA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# BERT Fine-Tuning on CoLA with SMART and SiFT\n",
        "\n",
        "This notebook was orginally created by Chris McCormick and Nick Ryan. We made changes for SiFT and SMART, as well as our custom BERT class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJKaoairpdRa"
      },
      "source": [
        "##Data and Importing Modules "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57c9d89-72d6-4e33-a02a-b21287c91117"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6ec749-13a0-4bf8-81fd-bc6a1cb01e4f"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea6813c-c1da-4f38-c220-2c359a2e2655"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bda35e-bc9e-43b8-fca9-f4051fa04591"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0957fdab-a0ee-4ad9-ae3b-35c5635a7c99"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv-tNv20dnH"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "d883ab2b-3c8b-4fe4-9064-0831395dd7e5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5278</th>\n",
              "      <td>b_82</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It was believed to have disturbed him that peo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7762</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's extremely windy today.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2593</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jessica sprayed paint over the table.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5252</th>\n",
              "      <td>b_82</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>He stopped writing poems.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The man with a book.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7830</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Because they hated him, the druids forced Jaso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3784</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John talked to Bill about the exam.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3844</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Doing syntax is not easy.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5582</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>People from Tucson think very highly of themse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8229</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I have been flying helicopters for years.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "5278            b_82  ...  It was believed to have disturbed him that peo...\n",
              "7762            ad03  ...                        It's extremely windy today.\n",
              "2593            l-93  ...              Jessica sprayed paint over the table.\n",
              "5252            b_82  ...                          He stopped writing poems.\n",
              "564             bc01  ...                               The man with a book.\n",
              "7830            ad03  ...  Because they hated him, the druids forced Jaso...\n",
              "3784            ks08  ...                John talked to Bill about the exam.\n",
              "3844            ks08  ...                          Doing syntax is not easy.\n",
              "5582            c_13  ...  People from Tucson think very highly of themse...\n",
              "8229            ad03  ...          I have been flying helicopters for years.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2f86d84d-3ab0-4778-d1cc-542497e97316"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>Handsome though I know several boys who are, I...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2827</th>\n",
              "      <td>The naughty child spanked.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1915</th>\n",
              "      <td>Bartlett and danced Toni.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8016</th>\n",
              "      <td>Jason intended for he to learn magic.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8394</th>\n",
              "      <td>There seem three men to be in the garden.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1748  Handsome though I know several boys who are, I...      0\n",
              "2827                         The naughty child spanked.      0\n",
              "1915                          Bartlett and danced Toni.      0\n",
              "8016              Jason intended for he to learn magic.      0\n",
              "8394          There seem three men to be in the garden.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFSJzwI5pujc"
      },
      "source": [
        "## Tokenization and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbab286-319a-4f59-95c9-4d7e94daef9a"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d656f657-2c26-483f-e137-1983aeeccd83"
      },
      "source": [
        "print(' Original: ', sentences[0])\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKsH2sU0OCQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae977e8a-8df5-4017-d7c3-f08e7f857ef0"
      },
      "source": [
        "max_len = 0\n",
        "for sent in sentences:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d5a35a-6c60-4f16-95f5-55ab85662a00"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30819f1c-3d24-456c-f34d-611d75494cd6"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73S4P4SMp6hX"
      },
      "source": [
        "## Custom Bert Class and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOteWAT-Adqx"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings,BertEncoder,BertPooler\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "class CustomBertForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        #self.bert = BertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = False)\n",
        "        self.embeddings = self.bert.embeddings\n",
        "        self.encoder = self.bert.encoder\n",
        "        self.pooler = self.bert.pooler\n",
        "\n",
        "    def embed(self, input_ids=None, \n",
        "                    token_type_ids=None, \n",
        "                    position_ids=None, \n",
        "                    inputs_embeds=None, \n",
        "                    past_key_values_length=0):\n",
        "        # See: BERTModel.forward\n",
        "        return self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length\n",
        "        )\n",
        "    \n",
        "    def predict(self,embedding_output,\n",
        "                extended_attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_extended_attention_mask=None,\n",
        "                past_key_values=None,\n",
        "                use_cache=None,\n",
        "                output_attentions=None,\n",
        "                output_hidden_states=None,\n",
        "                return_dict=True):\n",
        "      # See: BERTModel.forward \n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "        \n",
        "        bert_output = BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "                    last_hidden_state=sequence_output,\n",
        "                    pooler_output=pooled_output,\n",
        "                    past_key_values=encoder_outputs.past_key_values,\n",
        "                    hidden_states=encoder_outputs.hidden_states,\n",
        "                    attentions=encoder_outputs.attentions,\n",
        "                    cross_attentions=encoder_outputs.cross_attentions,\n",
        "                )\n",
        "\n",
        "        pooled_output = bert_output[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdNBO5qk2-i_",
        "collapsed": true,
        "outputId": "536d18eb-0349-43d0-c11d-3c8fcfa8ff6c"
      },
      "source": [
        "#@title\n",
        "model = CustomBertForClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing CustomBertForClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CustomBertForClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CustomBertForClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.layer.6.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomBertForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmSpMRD5qaqE"
      },
      "source": [
        "##Noise Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG5DszcpDAjw"
      },
      "source": [
        "from torch.nn import LayerNorm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def normalize_embed(embed):\n",
        "    embed_mean = torch.mean(embed,dim=(1,2))\n",
        "    embed_std = torch.std(embed, dim=(1,2))\n",
        "\n",
        "    embed_clone = torch.clone(embed)\n",
        "\n",
        "    for i in range(0,embed_clone.size()[0]):\n",
        "        # embed_clone[i] = torch.div(torch.sub(embed_clone[i],embed_mean[i]),embed_std[i])\n",
        "        embed_clone[i] = (embed_clone[i] - embed_mean[i]) / embed_std[i]\n",
        "    return embed_clone, embed_mean, embed_std\n",
        "\n",
        "def denormalize_embed(embed, embed_mean, embed_std):\n",
        "    for i in range(0,embed.size()[0]):\n",
        "        # embed[i] = (embed[i] - embed_mean[i]) / embed_std[i]\n",
        "        embed[i] = (embed[i] * embed_std[i]) + embed_mean[i]\n",
        "    return embed \n",
        "\n",
        "def stable_kl(logit, target, epsilon=1e-6, reduce=True):\n",
        "    logit = logit.view(-1, logit.size(-1)).float()\n",
        "    target = target.view(-1, target.size(-1)).float()\n",
        "    bs = logit.size(0)\n",
        "    p = F.log_softmax(logit, 1).exp()\n",
        "    y = F.log_softmax(target, 1).exp()\n",
        "    rp = -(1.0/(p + epsilon) -1 + epsilon).detach().log()\n",
        "    ry = -(1.0/(y + epsilon) -1 + epsilon).detach().log()\n",
        "    if reduce:\n",
        "        return (p* (rp- ry) * 2).sum() / bs\n",
        "    else:\n",
        "        return (p* (rp- ry) * 2).sum()\n",
        "\n",
        "def _norm_grad(grad, epsilon = 1e-6, eff_grad=None, sentence_level=False):\n",
        "        if sentence_level:\n",
        "            direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
        "        else:\n",
        "            direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "            eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "        return direction, eff_direction\n",
        "\n",
        "def noise(embed, model, step_size, normalize=False, k=1, mean=0, std=0.01):\n",
        "    if normalize == True:\n",
        "        logits = model.predict(embed)\n",
        "        # LNorm = LayerNorm(embed.size(),elementwise_affine=False)\n",
        "        # normalized_embed = LNorm(embed)\n",
        "        normalized_embed, embed_mean, embed_std = normalize_embed(embed)\n",
        "\n",
        "        noise = torch.normal(mean=0, std=0.01,size=(normalized_embed.size()[0],normalized_embed.size()[1],normalized_embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_normalized_embeddings = normalized_embed+noise\n",
        "        adv_logits = model.predict(noised_normalized_embeddings)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        noised_normalized_embeddings = normalized_embed+noise\n",
        "        denormalize_noised_embed = denormalize_embed(noised_normalized_embeddings,embed_mean, embed_std) \n",
        "        return denormalize_noised_embed\n",
        "    else:\n",
        "        logits = model.predict(embed)\n",
        "        noise = torch.normal(mean=0, std=0.01,size=(embed.size()[0],embed.size()[1],embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_embeddings = embed+noise\n",
        "        adv_logits = model.predict(noised_embeddings)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        noised_embeddings = embed+noise\n",
        "        return noised_embeddings\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bunW4qF4qSyZ"
      },
      "source": [
        "## Optimizer, Scheduler, and Some Other Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "#@title\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "#@title\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 6\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                          num_warmup_steps = 0,\n",
        "                          num_training_steps = total_steps\n",
        "                        )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "#@title\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjvBSBfHtBc"
      },
      "source": [
        "# MODE = \"SIFT\"\n",
        "MODE = \"SMART-adv-only\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSpuOXLqor-"
      },
      "source": [
        "##Training Loop with Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fca6c02-f5b5-48bc-bba9-68100e15239d"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        embed = model.embed(input_ids = b_input_ids)\n",
        "        preds = model.predict(embedding_output = embed)\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        regular_loss = loss_fct(preds.view(-1,2), b_labels.view(-1))\n",
        "        loss_list = [regular_loss]\n",
        "        if MODE in [\"SMART-adv-only\", \"SIFT\"]:\n",
        "          normalise = True if MODE == \"SIFT\" else False\n",
        "          noised_embeddings = noise(embed, model, 1e-3, normalize=normalise, k=1)\n",
        "          adv_logits = model.predict(embedding_output = noised_embeddings)\n",
        "\n",
        "          adv_loss = stable_kl(preds.view(-1,2), adv_logits.view(-1,2))\n",
        "          loss_list.append(adv_loss)\n",
        "        loss = sum(loss_list)\n",
        "        # END MODEL\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)   \n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:36.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:13.\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:14.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:01:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:14.\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:01:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:14.\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:01:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:14.\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:01:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:14.\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:01:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:07:28 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_NbXFGMukX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "3694a0fe-6351-4ed1-f75b-9fca4f694c63"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0:01:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.64</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0:01:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0:01:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0:01:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0:01:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.59</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:01:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.63         0.59           0.73       0:01:13         0:00:01\n",
              "2               0.64         0.50           0.82       0:01:14         0:00:01\n",
              "3               0.62         0.46           0.82       0:01:14         0:00:01\n",
              "4               0.60         0.46           0.82       0:01:14         0:00:01\n",
              "5               0.60         0.44           0.82       0:01:14         0:00:01\n",
              "6               0.59         0.44           0.83       0:01:14         0:00:01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "99f75fc9-8c20-4537-d782-ca108fa965fc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "##Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c94c4a4-5949-4f08-f314-66aee014af97"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent, \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,  \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   ) \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        " \n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "###Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d37218-b0de-49b2-9b27-a9614a369b8d"
      },
      "source": [
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jscIM8R4Gv"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97562ca-4bf1-4abc-a03e-dd2fe7d6f914"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3dc3b49-0520-4628-c963-95bea27d234c"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfY1tqxU0t9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "17a1e613-5d30-46d6-efac-e809fae264a2"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YrjAPX2V-l4"
      },
      "source": [
        "Now we'll combine the results for all of the batches and calculate our final MCC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e41ded-0029-4f7c-900e-a3288267d370"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd-LgEsq6dBh"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}