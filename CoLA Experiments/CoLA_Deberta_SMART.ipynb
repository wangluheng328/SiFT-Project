{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLA_Deberta_SMART.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# BERT Fine-Tuning on CoLA with SMART and SiFT\n",
        "\n",
        "This notebook was orginally created by Chris McCormick and Nick Ryan. We made changes for SiFT and SMART, as well as our custom BERT class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJKaoairpdRa"
      },
      "source": [
        "##Data and Importing Modules "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a780c179-3a77-42bd-fafb-90d1b1739f3a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51b0742-4ecd-4d5a-ff6c-db65ddbbf327"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4c7221-a2b6-4fc1-bdb4-639c26315c3f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0d2238-8ba9-4a93-c0f1-1581ed47ef9c"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9074338c-d4da-4ce7-9ae7-e8bf59d05bbe"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv-tNv20dnH"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3449cfb2-71a3-4393-fd34-79d200928ab8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Angela characterized Shelly as a lifesaver.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5048</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They're not finding it a stress being in the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Paul exhaled on Mary.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I ordered if John drink his beer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Press the stamp against the pad completely.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3542</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They can very.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6915</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This arch is supporting the weight of the tower.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>That new handle detaches easily.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5857</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Brazilians pumped the oil across the river.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It is a wooden desk.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2389            l-93  ...        Angela characterized Shelly as a lifesaver.\n",
              "5048            ks08  ...  They're not finding it a stress being in the s...\n",
              "3133            l-93  ...                              Paul exhaled on Mary.\n",
              "5955            c_13  ...                  I ordered if John drink his beer.\n",
              "625             bc01  ...        Press the stamp against the pad completely.\n",
              "3542            ks08  ...                                     They can very.\n",
              "6915            m_02  ...   This arch is supporting the weight of the tower.\n",
              "2908            l-93  ...                   That new handle detaches easily.\n",
              "5857            c_13  ...    The Brazilians pumped the oil across the river.\n",
              "4191            ks08  ...                               It is a wooden desk.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "80fe4d8b-e5a4-4a58-b0f1-61ba04dc64f5"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6770</th>\n",
              "      <td>We realised that Dr Jones died because he ate ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>Here's a pole for you to kiss the girl who tie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>Jennifer baked at the potatoes.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4651</th>\n",
              "      <td>Kim is resembled by the model in nearly every ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2672</th>\n",
              "      <td>The book sent to Peter.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "6770  We realised that Dr Jones died because he ate ...      0\n",
              "1652  Here's a pole for you to kiss the girl who tie...      0\n",
              "3258                    Jennifer baked at the potatoes.      0\n",
              "4651  Kim is resembled by the model in nearly every ...      0\n",
              "2672                            The book sent to Peter.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFSJzwI5pujc"
      },
      "source": [
        "## Tokenization and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4b6b23-94cb-4057-e343-919e14cffe7c"
      },
      "source": [
        "from transformers import DebertaTokenizer\n",
        "print('Loading DeBERTa tokenizer...')\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base', do_lower_case=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading DeBERTa tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506ff13e-0d94-417a-db98-d023e46cf777"
      },
      "source": [
        "print(' Original: ', sentences[0])\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['Our', 'Ġfriends', 'Ġwon', \"'t\", 'Ġbuy', 'Ġthis', 'Ġanalysis', ',', 'Ġlet', 'Ġalone', 'Ġthe', 'Ġnext', 'Ġone', 'Ġwe', 'Ġpropose', '.']\n",
            "Token IDs:  [2522, 964, 351, 75, 907, 42, 1966, 6, 905, 1937, 5, 220, 65, 52, 15393, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKsH2sU0OCQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caed30e4-0700-4b82-997b-b079d60551fc"
      },
      "source": [
        "max_len = 0\n",
        "for sent in sentences:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57225a57-e66b-4226-9c40-7cb46745a66c"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([    1,  2522,   964,   351,    75,   907,    42,  1966,     6,   905,\n",
            "         1937,     5,   220,    65,    52, 15393,     4,     2,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aec5da-0eb3-43ac-a738-b36eb54bc76f"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73S4P4SMp6hX"
      },
      "source": [
        "## Custom Deberta Class and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOteWAT-Adqx"
      },
      "source": [
        "from transformers import DebertaForSequenceClassification, AdamW, DebertaConfig, DebertaPreTrainedModel, DebertaModel\n",
        "from transformers.models.deberta.modeling_deberta import *\n",
        "#from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "class CustomDebertaForClassification(DebertaForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        #self.bert = BertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = False)\n",
        "        self.embeddings = self.deberta.embeddings\n",
        "        self.encoder = self.deberta.encoder\n",
        "        self.z_steps = 0 #copied from DebertaModel source code\n",
        "\n",
        "\n",
        "    def embed(self, input_ids=None, \n",
        "                    mask=None,\n",
        "                    token_type_ids=None, \n",
        "                    position_ids=None, \n",
        "                    inputs_embeds=None\n",
        "                    ):\n",
        "        # See: BERTModel.forward\n",
        "        return self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            mask=mask,\n",
        "            inputs_embeds=inputs_embeds\n",
        "        )\n",
        "    \n",
        "    def predict(self,embedding_output,\n",
        "                attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_extended_attention_mask=None,\n",
        "                past_key_values=None,\n",
        "                use_cache=None,\n",
        "                output_attentions=None,\n",
        "                output_hidden_states=None,\n",
        "                return_dict=True): \n",
        "        encoder_outputs = self.encoder(\n",
        "                                        embedding_output,\n",
        "                                        attention_mask,\n",
        "                                        output_hidden_states=True,\n",
        "                                        output_attentions=output_attentions,\n",
        "                                        return_dict=return_dict\n",
        "        )\n",
        "        encoded_layers = encoder_outputs[1]\n",
        "\n",
        "        if self.z_steps > 1:\n",
        "            hidden_states = encoded_layers[-2]\n",
        "            layers = [self.encoder.layer[-1] for _ in range(self.z_steps)]\n",
        "            query_states = encoded_layers[-1]\n",
        "            rel_embeddings = self.encoder.get_rel_embedding()\n",
        "            attention_mask = self.encoder.get_attention_mask(attention_mask)\n",
        "            rel_pos = self.encoder.get_rel_pos(embedding_output)\n",
        "            for layer in layers[1:]:\n",
        "                query_states = layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    return_att=False,\n",
        "                    query_states=query_states,\n",
        "                    relative_pos=rel_pos,\n",
        "                    rel_embeddings=rel_embeddings,\n",
        "                )\n",
        "                encoded_layers.append(query_states)\n",
        "\n",
        "        sequence_output = encoded_layers[-1]\n",
        "\n",
        "        # if not return_dict:\n",
        "        #     return (sequence_output,) + encoder_outputs[(1 if output_hidden_states else 2) :]\n",
        "\n",
        "        outputs = BaseModelOutput(\n",
        "            last_hidden_state=sequence_output,\n",
        "            hidden_states=encoder_outputs.hidden_states if output_hidden_states else None,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        pooled_output = self.pooler(outputs[0])\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdNBO5qk2-i_",
        "collapsed": true,
        "outputId": "d5763076-07a6-42a8-fc83-b9daa60e14e3"
      },
      "source": [
        "#@title\n",
        "model = CustomDebertaForClassification.from_pretrained(\n",
        "    \"microsoft/deberta-base\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing CustomDebertaForClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
            "- This IS expected if you are initializing CustomDebertaForClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CustomDebertaForClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CustomDebertaForClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['encoder.layer.5.intermediate.dense.weight', 'classifier.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.self.pos_proj.weight', 'encoder.layer.9.attention.self.pos_q_proj.bias', 'encoder.layer.5.attention.self.pos_proj.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.pos_q_proj.bias', 'embeddings.LayerNorm.bias', 'classifier.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.attention.self.pos_q_proj.bias', 'encoder.layer.3.attention.self.q_bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.pos_q_proj.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.self.pos_proj.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.0.attention.self.q_bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.pos_q_proj.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.pos_q_proj.weight', 'encoder.layer.5.attention.self.in_proj.weight', 'encoder.layer.7.attention.self.pos_q_proj.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.in_proj.weight', 'encoder.layer.1.attention.self.in_proj.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.in_proj.weight', 'encoder.layer.10.attention.self.q_bias', 'encoder.layer.9.attention.self.in_proj.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.attention.self.in_proj.weight', 'encoder.layer.6.attention.self.pos_q_proj.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.pos_proj.weight', 'encoder.layer.6.attention.self.pos_q_proj.weight', 'encoder.layer.6.attention.self.in_proj.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.pos_q_proj.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.pos_q_proj.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.self.pos_proj.weight', 'encoder.layer.5.attention.self.pos_q_proj.weight', 'encoder.layer.8.attention.self.v_bias', 'encoder.layer.11.attention.self.in_proj.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.self.pos_q_proj.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.q_bias', 'encoder.layer.2.attention.self.pos_proj.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.rel_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.3.attention.self.in_proj.weight', 'encoder.layer.9.attention.self.q_bias', 'encoder.layer.5.attention.self.pos_q_proj.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.pos_q_proj.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.v_bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.attention.self.in_proj.weight', 'encoder.layer.5.attention.self.q_bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.v_bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.self.q_bias', 'encoder.layer.10.attention.self.pos_proj.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.v_bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.q_bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.pos_q_proj.bias', 'pooler.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.v_bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.self.in_proj.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.pos_q_proj.weight', 'encoder.layer.10.attention.self.pos_q_proj.bias', 'encoder.layer.11.attention.self.q_bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.pos_q_proj.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.pos_q_proj.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.attention.self.pos_q_proj.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.pos_q_proj.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.v_bias', 'encoder.layer.4.attention.self.in_proj.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.q_bias', 'encoder.layer.1.attention.self.pos_proj.weight', 'encoder.layer.2.attention.self.v_bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.q_bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.self.pos_proj.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.pos_proj.weight', 'encoder.layer.10.attention.self.v_bias', 'encoder.layer.4.attention.self.v_bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.self.q_bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.attention.self.v_bias', 'encoder.layer.7.attention.self.pos_q_proj.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.pos_proj.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.pos_q_proj.weight', 'encoder.layer.7.attention.self.v_bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.pos_proj.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.self.v_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomDebertaForClassification(\n",
              "  (deberta): DebertaModel(\n",
              "    (embeddings): DebertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "      (LayerNorm): DebertaLayerNorm()\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (1): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (2): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (3): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (4): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (5): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (6): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (7): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (8): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (9): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (10): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (11): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(1024, 768)\n",
              "    )\n",
              "  )\n",
              "  (pooler): ContextPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): StableDropout()\n",
              "  (embeddings): DebertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "    (LayerNorm): DebertaLayerNorm()\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (encoder): DebertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (1): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (2): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (3): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (4): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (5): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (6): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (7): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (8): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (9): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (10): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "      (11): DebertaLayer(\n",
              "        (attention): DebertaAttention(\n",
              "          (self): DisentangledSelfAttention(\n",
              "            (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (pos_dropout): StableDropout()\n",
              "            (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "          (output): DebertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (intermediate): DebertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): DebertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): DebertaLayerNorm()\n",
              "          (dropout): StableDropout()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (rel_embeddings): Embedding(1024, 768)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmSpMRD5qaqE"
      },
      "source": [
        "##Noise Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG5DszcpDAjw"
      },
      "source": [
        "from torch.nn import LayerNorm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def normalize_embed(embed):\n",
        "    embed_mean = torch.mean(embed,dim=(1,2))\n",
        "    embed_std = torch.std(embed, dim=(1,2))\n",
        "\n",
        "    embed_clone = torch.clone(embed)\n",
        "\n",
        "    for i in range(0,embed_clone.size()[0]):\n",
        "        # embed_clone[i] = torch.div(torch.sub(embed_clone[i],embed_mean[i]),embed_std[i])\n",
        "        embed_clone[i] = (embed_clone[i] - embed_mean[i]) / embed_std[i]\n",
        "    return embed_clone, embed_mean, embed_std\n",
        "\n",
        "def denormalize_embed(embed, embed_mean, embed_std):\n",
        "    for i in range(0,embed.size()[0]):\n",
        "        # embed[i] = (embed[i] - embed_mean[i]) / embed_std[i]\n",
        "        embed[i] = (embed[i] * embed_std[i]) + embed_mean[i]\n",
        "    return embed \n",
        "\n",
        "def stable_kl(logit, target, epsilon=1e-6, reduce=True):\n",
        "    logit = logit.view(-1, logit.size(-1)).float()\n",
        "    target = target.view(-1, target.size(-1)).float()\n",
        "    bs = logit.size(0)\n",
        "    p = F.log_softmax(logit, 1).exp()\n",
        "    y = F.log_softmax(target, 1).exp()\n",
        "    rp = -(1.0/(p + epsilon) -1 + epsilon).detach().log()\n",
        "    ry = -(1.0/(y + epsilon) -1 + epsilon).detach().log()\n",
        "    if reduce:\n",
        "        return (p* (rp- ry) * 2).sum() / bs\n",
        "    else:\n",
        "        return (p* (rp- ry) * 2).sum()\n",
        "\n",
        "def _norm_grad(grad, epsilon = 1e-6, eff_grad=None, sentence_level=False):\n",
        "        if sentence_level:\n",
        "            direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
        "        else:\n",
        "            direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "            eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "        return direction, eff_direction\n",
        "\n",
        "def noise(embed, model, attention_mask, step_size, normalize=False, k=1, mean=0, std=0.01):\n",
        "    if normalize == True:\n",
        "        logits = model.predict(embed,attention_mask)\n",
        "        # LNorm = LayerNorm(embed.size(),elementwise_affine=False)\n",
        "        # normalized_embed = LNorm(embed)\n",
        "        normalized_embed, embed_mean, embed_std = normalize_embed(embed)\n",
        "        \n",
        "        noise = torch.normal(mean=0, std=0.01,size=(normalized_embed.size()[0],normalized_embed.size()[1],normalized_embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_normalized_embeddings = normalized_embed+noise\n",
        "        adv_logits = model.predict(noised_normalized_embeddings, attention_mask)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        noised_normalized_embeddings = normalized_embed+noise\n",
        "        denormalize_noised_embed = denormalize_embed(noised_normalized_embeddings,embed_mean, embed_std) \n",
        "        return denormalize_noised_embed\n",
        "\n",
        "    else:\n",
        "        logits = model.predict(embed,attention_mask)\n",
        "        noise = torch.normal(mean=0, std=0.01,size=(embed.size()[0],embed.size()[1],embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_embeddings = embed+noise\n",
        "        adv_logits = model.predict(noised_embeddings, attention_mask)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        noised_embeddings = embed+noise\n",
        "        return noised_embeddings\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bunW4qF4qSyZ"
      },
      "source": [
        "## Optimizer, Scheduler, and Some Other Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "#@title\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "#@title\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 6\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                          num_warmup_steps = 0,\n",
        "                          num_training_steps = total_steps\n",
        "                        )"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "#@title\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjvBSBfHtBc"
      },
      "source": [
        "MODE = \"SMART-adv-only\""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSpuOXLqor-"
      },
      "source": [
        "##Training Loop with Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3007645-eace-457d-e3a4-a3f49471d31e"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        embed = model.embed(input_ids = b_input_ids,mask = b_input_mask)\n",
        "        preds = model.predict(embedding_output = embed,attention_mask = b_input_mask)\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        regular_loss = loss_fct(preds.view(-1,2), b_labels.view(-1))\n",
        "        loss_list = [regular_loss]\n",
        "        if MODE in [\"SMART-adv-only\", \"SIFT\"]:\n",
        "          normalise = True if MODE == \"SIFT\" else False\n",
        "          noised_embeddings = noise(embed, model, b_input_mask, 1e-3, normalize=normalise, k=1)\n",
        "          adv_logits = model.predict(noised_embeddings, b_input_mask)\n",
        "\n",
        "          adv_loss = stable_kl(preds.view(-1,2), adv_logits.view(-1,2))\n",
        "          loss_list.append(adv_loss)\n",
        "        loss = sum(loss_list)\n",
        "        # END MODEL\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)   \n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epcoh took: 0:02:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:02:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.38\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:02:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:02:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:02:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 6 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   120  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:44.\n",
            "  Batch   200  of    241.    Elapsed: 0:02:10.\n",
            "  Batch   240  of    241.    Elapsed: 0:02:36.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:02:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:15:51 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_NbXFGMukX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "fde4715e-5806-49c4-9bd7-3f38c05380d9"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:02:37</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0:02:37</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:37</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:02:36</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0:02:36</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:02:36</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.63         0.45           0.81       0:02:37         0:00:02\n",
              "2               0.57         0.38           0.85       0:02:37         0:00:02\n",
              "3               0.53         0.36           0.84       0:02:37         0:00:02\n",
              "4               0.51         0.37           0.84       0:02:36         0:00:02\n",
              "5               0.49         0.36           0.85       0:02:36         0:00:02\n",
              "6               0.47         0.35           0.86       0:02:36         0:00:02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "42731272-75ff-4c21-978e-0fc20b7b2c98"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "##Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab55a86-b63e-4b4a-a299-6930140f6689"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent, \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,  \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   ) \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        " \n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "###Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af4c358-b6db-4ab1-99d5-f34c94eed5ad"
      },
      "source": [
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jscIM8R4Gv"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ed786d-244c-4b5c-b7f1-66da0421eaf0"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf52dee-6d01-44a9-da8b-944d50762c51"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfY1tqxU0t9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "1ed003c9-b18a-45b6-d336-6c14f3860609"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YrjAPX2V-l4"
      },
      "source": [
        "Now we'll combine the results for all of the batches and calculate our final MCC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d6e2d6-55d2-49bc-c434-42694e43e1f8"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.576\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}