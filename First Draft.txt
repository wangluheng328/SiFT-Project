Intro


Overfitting is still a major problem for fine-tuning. We are intrigued by a solution proposed in the DeBERTa paper - the SiFT algorithm. The SiFT algorithm is based on the SMART algorithm, which performs smoothness-inducing adversarial training to the word embeddings. It uses KL Divergence to measure and limit the impact of perturbation on the final output, and Bregman Proximal Point Optimization to prevent aggressive update. What distinguishes SiFT from SMART is that during fine-tuning, SiFT lets the embeddings go through a normalization step before being perturbed. On the contrary, in SMART they are first perturbed and then normalized. We believe this will make a difference, because in this way the embeddings no longer follow a gaussian distribution before being fed into the model. In other words, the power of perturbation is more explicit. Our project is based on this specific solution of model robustness. The authors of DeBERTa claim in the paper that a more comprehensive study of SiFT is left to future scholars. We decide we can explore the significance of the SiFT algorithm on various tasks and models. We do this by establishing our baseline models: BERT, RoBERTa, and DeBERTa. We finetune these baseline models on various tasks like COLA, UCI Sentiment, etc (explained in more detail below). We record their performance as our baseline results. We then, based on SMART, implement our own version of SiFT on these models and evaluate the results.