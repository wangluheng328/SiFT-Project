{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f276fa8370f84ab8ba4ea617fe0df7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_558ea0107f3946ea80419f51d4983756",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb4fd88998a74568b8ca3dfc52d8a2c9",
              "IPY_MODEL_fc49a3fd726d4c6bb1ceb6f36f13e5a1"
            ]
          }
        },
        "558ea0107f3946ea80419f51d4983756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb4fd88998a74568b8ca3dfc52d8a2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dab98e7bfa3a47a3b009a315ffdb57cf",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c91c4540a4024f5cb786fc8f6fc36025"
          }
        },
        "fc49a3fd726d4c6bb1ceb6f36f13e5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b5aa9241c0e4af8be566e4517782303",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 2.11kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1becf18ec57f4142bb7182b5bca9a187"
          }
        },
        "dab98e7bfa3a47a3b009a315ffdb57cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c91c4540a4024f5cb786fc8f6fc36025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b5aa9241c0e4af8be566e4517782303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1becf18ec57f4142bb7182b5bca9a187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5963514dd5746b095c136c70f1cfae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b5a7b06843f4fde8308e525a6f3e982",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eeb08c017945462194d6bf12715c0fd8",
              "IPY_MODEL_be859d88b6174116b0ba495812a9a25d"
            ]
          }
        },
        "3b5a7b06843f4fde8308e525a6f3e982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eeb08c017945462194d6bf12715c0fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_098e5dd7550c4b4bb98698cda68541ac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_106e6baafd974fefa86779f3664c89be"
          }
        },
        "be859d88b6174116b0ba495812a9a25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_656bd5730bfd47faab57de33cc68d85f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:22&lt;00:00, 19.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a95fcdafed34415a4342322527463db"
          }
        },
        "098e5dd7550c4b4bb98698cda68541ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "106e6baafd974fefa86779f3664c89be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "656bd5730bfd47faab57de33cc68d85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a95fcdafed34415a4342322527463db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# BERT Fine-Tuning Tutorial with PyTorch\n",
        "\n",
        "By Chris McCormick and Nick Ryan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40c30c3-652b-404b-c36a-0d68fe02e070"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56133f92-4a02-43fe-dcfb-97beb7aa804b"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d75e33b-c5db-4619-b777-1985964a6b20"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf2e28e-27e4-41d7-a7fe-b744898eb16e"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8559843-9c05-4dce-dc26-97af5738bcb6"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv-tNv20dnH"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "e62923b1-2e15-461f-eed7-1ea7f39ebfb1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4376</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John couldn't leave the party.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7990</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Which poisonous plant is it certain that we wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8488</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Medea tried Medea to poison her children.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4170</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>How much fresco did the flood damage?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The terrier attacked the burglar and savaged t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Each other's houses proved to the women that t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8535</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Anson demonized David every day.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1859</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Them, they can't stand each other.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6623</th>\n",
              "      <td>m_02</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>This truck spread fewer salt than that one.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I only eat fish drunk raw.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "4376            ks08  ...                     John couldn't leave the party.\n",
              "7990            ad03  ...  Which poisonous plant is it certain that we wi...\n",
              "8488            ad03  ...          Medea tried Medea to poison her children.\n",
              "4170            ks08  ...              How much fresco did the flood damage?\n",
              "6668            m_02  ...  The terrier attacked the burglar and savaged t...\n",
              "1070            bc01  ...  Each other's houses proved to the women that t...\n",
              "8535            ad03  ...                   Anson demonized David every day.\n",
              "1859            r-67  ...                 Them, they can't stand each other.\n",
              "6623            m_02  ...        This truck spread fewer salt than that one.\n",
              "843             bc01  ...                         I only eat fish drunk raw.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "9bce8693-5bb5-436c-868d-5b24e70792d1"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5610</th>\n",
              "      <td>I expect to double my profits more than.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2123</th>\n",
              "      <td>I sent the devil the salesman.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4377</th>\n",
              "      <td>John leftn't the party early.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4373</th>\n",
              "      <td>Tom kicked not a ball.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4901</th>\n",
              "      <td>The student met John came.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      sentence  label\n",
              "5610  I expect to double my profits more than.      0\n",
              "2123            I sent the devil the salesman.      0\n",
              "4377             John leftn't the party early.      0\n",
              "4373                    Tom kicked not a ball.      0\n",
              "4901                The student met John came.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2561f27-1ce2-403d-a0c4-b0eb002554a5"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ffb761-7139-4f14-ee16-409378e1f6e1"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKsH2sU0OCQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885bd93e-2acf-4fef-c218-3b845f23a400"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0180fd67-2496-4a62-e0b0-d20e4a3ea8a9"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a943cb19-0af9-4086-9f39-b01eb7ad7b68"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOteWAT-Adqx"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings,BertEncoder,BertPooler\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "class CustomBertForClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        #self.bert = BertForSequenceClassification(config).from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False, output_hidden_states = False)\n",
        "        self.embeddings = self.bert.embeddings\n",
        "        self.encoder = self.bert.encoder\n",
        "        self.pooler = self.bert.pooler\n",
        "\n",
        "    def embed(self, input_ids=None, \n",
        "                    token_type_ids=None, \n",
        "                    position_ids=None, \n",
        "                    inputs_embeds=None, \n",
        "                    past_key_values_length=0):\n",
        "        # See: BERTModel.forward \n",
        "        # ??? Some prep code\n",
        "        return self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length\n",
        "        )\n",
        "    \n",
        "    def predict(self,embedding_output,\n",
        "                extended_attention_mask=None,\n",
        "                head_mask=None,\n",
        "                encoder_hidden_states=None,\n",
        "                encoder_extended_attention_mask=None,\n",
        "                past_key_values=None,\n",
        "                use_cache=None,\n",
        "                output_attentions=None,\n",
        "                output_hidden_states=None,\n",
        "                return_dict=True):\n",
        "      # See: BERTModel.forward \n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "        \n",
        "        bert_output = BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "                    last_hidden_state=sequence_output,\n",
        "                    pooler_output=pooled_output,\n",
        "                    past_key_values=encoder_outputs.past_key_values,\n",
        "                    hidden_states=encoder_outputs.hidden_states,\n",
        "                    attentions=encoder_outputs.attentions,\n",
        "                    cross_attentions=encoder_outputs.cross_attentions,\n",
        "                )\n",
        "\n",
        "        pooled_output = bert_output[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        return logits\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f276fa8370f84ab8ba4ea617fe0df7f0",
            "558ea0107f3946ea80419f51d4983756",
            "eb4fd88998a74568b8ca3dfc52d8a2c9",
            "fc49a3fd726d4c6bb1ceb6f36f13e5a1",
            "dab98e7bfa3a47a3b009a315ffdb57cf",
            "c91c4540a4024f5cb786fc8f6fc36025",
            "3b5aa9241c0e4af8be566e4517782303",
            "1becf18ec57f4142bb7182b5bca9a187",
            "b5963514dd5746b095c136c70f1cfae2",
            "3b5a7b06843f4fde8308e525a6f3e982",
            "eeb08c017945462194d6bf12715c0fd8",
            "be859d88b6174116b0ba495812a9a25d",
            "098e5dd7550c4b4bb98698cda68541ac",
            "106e6baafd974fefa86779f3664c89be",
            "656bd5730bfd47faab57de33cc68d85f",
            "3a95fcdafed34415a4342322527463db"
          ]
        },
        "id": "IdNBO5qk2-i_",
        "collapsed": true,
        "outputId": "f83684e0-0442-471c-d0d4-e464eb41ae9c"
      },
      "source": [
        "#@title\n",
        "model = CustomBertForClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f276fa8370f84ab8ba4ea617fe0df7f0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5963514dd5746b095c136c70f1cfae2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing CustomBertForClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CustomBertForClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CustomBertForClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomBertForClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "#@title\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "#@title\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "#@title\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG5DszcpDAjw"
      },
      "source": [
        "from torch.nn import LayerNorm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def stable_kl(logit, target, epsilon=1e-6, reduce=True):\n",
        "    logit = logit.view(-1, logit.size(-1)).float()\n",
        "    target = target.view(-1, target.size(-1)).float()\n",
        "    bs = logit.size(0)\n",
        "    p = F.log_softmax(logit, 1).exp()\n",
        "    y = F.log_softmax(target, 1).exp()\n",
        "    rp = -(1.0/(p + epsilon) -1 + epsilon).detach().log()\n",
        "    ry = -(1.0/(y + epsilon) -1 + epsilon).detach().log()\n",
        "    if reduce:\n",
        "        return (p* (rp- ry) * 2).sum() / bs\n",
        "    else:\n",
        "        return (p* (rp- ry) * 2).sum()\n",
        "\n",
        "def _norm_grad(grad, epsilon = 1e-6, eff_grad=None, sentence_level=False):\n",
        "        if sentence_level:\n",
        "            direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
        "        else:\n",
        "            direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "            eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
        "        return direction, eff_direction\n",
        "\n",
        "def noise(embed, model, step_size, normalize=False, k=1, mean=0, std=0.01):\n",
        "    if normalize == True:\n",
        "        logits = model.predict(embed)\n",
        "        LNorm = LayerNorm(embed.size())\n",
        "        normalized_embed = LNorm(embed)\n",
        "        \n",
        "        noise = torch.normal(mean=0, std=0.01,size=(normalized_embed.size()[0],normalized_embed.size()[1],normalized_embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_normalized_embeddings = normalized_embed+noise\n",
        "        adv_logits = model.predict(noised_embeddings)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        \n",
        "        return noised_normalized_embeddings\n",
        "\n",
        "    else:\n",
        "        logits = model.predict(embed)\n",
        "        noise = torch.normal(mean=0, std=0.01,size=(embed.size()[0],embed.size()[1],embed.size()[2]))\n",
        "        noise = noise.to(device)\n",
        "        noise.requires_grad_()\n",
        "        noised_embeddings = embed+noise\n",
        "        adv_logits = model.predict(noised_embeddings)\n",
        "        adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
        "        delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
        "        norm = delta_grad.norm()\n",
        "        # if (torch.isnan(norm) or torch.isinf(norm)):\n",
        "        #     return 0\n",
        "        eff_delta_grad = delta_grad * step_size\n",
        "        delta_grad = noise + delta_grad * step_size\n",
        "        noise, eff_noise = _norm_grad(delta_grad, eff_grad=eff_delta_grad, sentence_level=0)\n",
        "        noise = noise.detach()\n",
        "        \n",
        "        return noised_embeddings\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjvBSBfHtBc"
      },
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# from torch.nn import KLDivLoss\n",
        "MODE = \"SMART-adv-only\"\n",
        "\n",
        "# for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "#         # Progress update every 40 batches.\n",
        "#         if step  == 1:\n",
        "#             break\n",
        "\n",
        "#         b_input_ids = batch[0].to(device)\n",
        "#         b_input_mask = batch[1].to(device)\n",
        "#         b_labels = batch[2].to(device)\n",
        "\n",
        "      \n",
        "# embed = model.embed(input_ids = b_input_ids)\n",
        "# preds = model.predict(embedding_output = embed)\n",
        "# loss_fct = CrossEntropyLoss()\n",
        "# regular_loss = loss_fct(preds.view(-1,2), b_labels.view(-1))\n",
        "# loss_list = [regular_loss]\n",
        "# if MODE in [\"SMART-adv-only\", \"SIFT\"]:\n",
        "#         normalise = True if MODE == \"SIFT\" else False\n",
        "#         noised_embeddings = noise(embed, model, 1e-3, normalize=normalise, k=1)\n",
        "#         adv_logits = model.predict(embedding_output = noised_embeddings)\n",
        "\n",
        "#         adv_loss = stable_kl(preds.view(-1,2), adv_logits.view(-1,2))\n",
        "#         loss_list.append(adv_loss)\n",
        "# loss = sum(loss_list)\n",
        "# # END MODEL\n",
        "# loss.backward()\n",
        "# optimizer.step()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLFhvWR02Dyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc2f896-e358-48d5-ac93-f9c1f1dd2215"
      },
      "source": [
        "device"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f275a20-2bce-4257-f95e-9a4da6a6584b"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        embed = model.embed(input_ids = b_input_ids)\n",
        "        preds = model.predict(embedding_output = embed)\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        regular_loss = loss_fct(preds.view(-1,2), b_labels.view(-1))\n",
        "        loss_list = [regular_loss]\n",
        "        if MODE in [\"SMART-adv-only\", \"SIFT\"]:\n",
        "          normalise = True if MODE == \"SIFT\" else False\n",
        "          noised_embeddings = noise(embed, model, 1e-3, normalize=normalise, k=1)\n",
        "          adv_logits = model.predict(embedding_output = noised_embeddings)\n",
        "\n",
        "          adv_loss = stable_kl(preds.view(-1,2), adv_logits.view(-1,2))\n",
        "          loss_list.append(adv_loss)\n",
        "        loss = sum(loss_list)\n",
        "        # END MODEL\n",
        "        loss.backward()\n",
        "        \n",
        "        \n",
        "\n",
        "        # # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # # function and pass down the arguments. The `forward` function is \n",
        "        # # documented here: \n",
        "        # # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # # The results are returned in a results object, documented here:\n",
        "        # # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # # \"logits\"--the model outputs prior to activation.\n",
        "\n",
        "\n",
        "        # result = model(b_input_ids, \n",
        "        #                token_type_ids=None, \n",
        "        #                attention_mask=b_input_mask, \n",
        "        #                labels=b_labels,\n",
        "        #                return_dict=True)\n",
        "\n",
        "        # loss = result.loss\n",
        "        # logits = result.logits\n",
        "\n",
        "        # # Accumulate the training loss over all of the batches so that we can\n",
        "        # # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # # single value; the `.item()` function just returns the Python value \n",
        "        # # from the tensor.\n",
        "        # total_train_loss += loss.item()\n",
        "\n",
        "        # # Perform a backward pass to calculate the gradients.\n",
        "        # loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:43.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:23.\n",
            "  Batch   120  of    241.    Elapsed: 0:02:05.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:46.\n",
            "  Batch   200  of    241.    Elapsed: 0:03:27.\n",
            "  Batch   240  of    241.    Elapsed: 0:04:09.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:04:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:41.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   120  of    241.    Elapsed: 0:02:04.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:45.\n",
            "  Batch   200  of    241.    Elapsed: 0:03:26.\n",
            "  Batch   240  of    241.    Elapsed: 0:04:08.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:04:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:41.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:22.\n",
            "  Batch   120  of    241.    Elapsed: 0:02:03.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:45.\n",
            "  Batch   200  of    241.    Elapsed: 0:03:26.\n",
            "  Batch   240  of    241.    Elapsed: 0:04:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:04:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:41.\n",
            "  Batch    80  of    241.    Elapsed: 0:01:23.\n",
            "  Batch   120  of    241.    Elapsed: 0:02:04.\n",
            "  Batch   160  of    241.    Elapsed: 0:02:45.\n",
            "  Batch   200  of    241.    Elapsed: 0:03:26.\n",
            "  Batch   240  of    241.    Elapsed: 0:04:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:16:46 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_NbXFGMukX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "f69b875b-0b15-4d35-fe50-4209ee8e042b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0:04:09</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:04:08</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:04:08</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:04:07</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1                0.0         0.48           0.79       0:04:09         0:00:04\n",
              "2                0.0         0.45           0.80       0:04:08         0:00:04\n",
              "3                0.0         0.46           0.80       0:04:08         0:00:04\n",
              "4                0.0         0.44           0.80       0:04:07         0:00:04"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI"
      },
      "source": [
        "Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it's over-fitting on the training data. \n",
        "\n",
        "(For reference, we are using 7,695 training samples and 856 validation samples).\n",
        "\n",
        "Validation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on. \n",
        "\n",
        "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "c445232a-ef21-4fe9-ee94-d344ffe3bcd9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82e5310-560c-416d-b197-05e0a6d356c3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c995db2-af1b-4f09-8bd6-3c55b90ce5a3"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jscIM8R4Gv"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484ee6ee-0c25-4e10-f576-6e7adb300bfa"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1cd72c-60bb-4adc-8a69-262d5e78ebb6"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUM0UA1qJaVB"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfY1tqxU0t9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "b55cb813-5aac-4ece-df80-9cd27c4f9ca5"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YrjAPX2V-l4"
      },
      "source": [
        "Now we'll combine the results for all of the batches and calculate our final MCC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7652e63f-594c-46ce-ca09-cf6d87e3e8bb"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXx0jPc4HUfZ"
      },
      "source": [
        "Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. \n",
        "\n",
        "> *Note: To maximize the score, we should remove the \"validation set\" (which we used to help determine how many epochs to train for) and train on the entire training set.*\n",
        "\n",
        "The library documents the expected accuracy for this benchmark [here](https://huggingface.co/transformers/examples.html#glue) as `49.23`.\n",
        "\n",
        "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). \n",
        "\n",
        "Note that (due to the small dataset size?) the accuracy can vary significantly between runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlQG7qgkmf4n"
      },
      "source": [
        "This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO"
      },
      "source": [
        "# Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8"
      },
      "source": [
        "## A1. Saving & Loading Fine-Tuned Model\n",
        "\n",
        "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fe7902-2686-423c-9e81-ad0af58a64f4"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/tokenizer_config.json',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/vocab.txt',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a8072a-9437-40d4-9df9-bc22c3f42912"
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 428044K\n",
            "-rw-r--r-- 1 root root      1K May 10 16:30 config.json\n",
            "-rw-r--r-- 1 root root 427800K May 10 16:30 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K May 10 16:30 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K May 10 16:30 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K May 10 16:30 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23eb1793-9242-4d50-a6c8-74225941d581"
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M May 10 16:30 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr-A-POC18_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "0d72a935-4c13-4ed0-d6d9-3b10c8b5d3a1"
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-54350db54bed>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    drive.mount('/content/drive')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5"
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vstijw85SZ"
      },
      "source": [
        "The following functions will load the model back from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL"
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIWouvDrGVAi"
      },
      "source": [
        "## A.2. Weight Decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f123ZAlF1OyW"
      },
      "source": [
        "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix.\n",
        "\n",
        "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "source": [
        "# This code is taken from:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
        "\n",
        "# Don't apply weight decay to any parameters whose names include these tokens.\n",
        "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# Separate the `weight` parameters from the `bias` parameters. \n",
        "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "    \n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
        "# the names."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKzLS9ohzGVu"
      },
      "source": [
        "# Revision History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZqpiHEnGqYR"
      },
      "source": [
        "**Version 4** - *Feb 2nd, 2020* - (current)\n",
        "* Updated all calls to `model` (fine-tuning and evaluation) to use the [`SequenceClassifierOutput`](https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput) class.\n",
        "* Moved illustration images to Google Drive--Colab appears to no longer support images at external URLs.\n",
        "\n",
        "**Version 3** - *Mar 18th, 2020*\n",
        "* Simplified the tokenization and input formatting (for both training and test) by leveraging the `tokenizer.encode_plus` function. \n",
        "`encode_plus` handles padding *and* creates the attention masks for us.\n",
        "* Improved explanation of attention masks.\n",
        "* Switched to using `torch.utils.data.random_split` for creating the training-validation split.\n",
        "* Added a summary table of the training statistics (validation loss, time per epoch, etc.).\n",
        "* Added validation loss to the learning curve plot, so we can see if we're overfitting. \n",
        "    * Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing this!\n",
        "* Displayed the per-batch MCC as a bar plot.\n",
        "\n",
        "**Version 2** - *Dec 20th, 2019* - [link](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP)\n",
        "* huggingface renamed their library to `transformers`. \n",
        "* Updated the notebook to use the `transformers` library.\n",
        "\n",
        "**Version 1** - *July 22nd, 2019*\n",
        "* Initial version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_NnDGxRpEI"
      },
      "source": [
        "## Further Work\n",
        "\n",
        "* It might make more sense to use the MCC score for “validation accuracy”, but I’ve left it out so as not to have to explain it earlier in the Notebook.\n",
        "* Seeding -- I’m not convinced that setting the seed values at the beginning of the training loop is actually creating reproducible results…\n",
        "* The MCC score seems to vary substantially across different runs. It would be interesting to run this example a number of times and show the variance.\n"
      ]
    }
  ]
}